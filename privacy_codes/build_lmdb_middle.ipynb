{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:      Linux-3.13.0-65-generic-x86_64-with-Ubuntu-14.04-trusty\n",
      "Python:  2.7.6 (default, Jun 22 2015, 17:58:13) \n",
      "CUDA:    Cuda compilation tools, release 7.5, V7.5.17\n",
      "LMDB:    0.9.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "import lmdb\n",
    "import Image\n",
    "import sys\n",
    "import copy \n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "from StringIO import StringIO\n",
    "from caffe.proto import caffe_pb2\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats.mstats import rankdata\n",
    "from sklearn.externals import joblib\n",
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import caffe\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)\n",
    "\n",
    "print \"OS:     \", platform.platform()\n",
    "print \"Python: \", sys.version.split(\"\\n\")[0]\n",
    "print \"CUDA:   \", subprocess.Popen([\"nvcc\",\"--version\"], stdout=subprocess.PIPE).communicate()[0].split(\"\\n\")[3]\n",
    "print \"LMDB:   \", \".\".join([str(i) for i in lmdb.version()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv5_3/siamese/\n"
     ]
    }
   ],
   "source": [
    "kind = 'siamese'\n",
    "mid_layer = 'conv5_3'\n",
    "next_layer = 'pool5'\n",
    "addr = mid_layer+'/'+kind+'/'\n",
    "print addr\n",
    "num_pca = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_all_middle_features(net, middle_layer_name, num_batch, istest):\n",
    "    # Loading all features from middle layer\n",
    "    # num_batch is not the batch_size. (batch_size is set before in training)\n",
    "    features = []\n",
    "    labels = []\n",
    "    acc = 0\n",
    "    middle_layer_shape = net.blobs[middle_layer_name].data.shape\n",
    "    for i in range(0, num_batch):\n",
    "        out = net.forward(blobs=[middle_layer_name,'label'])\n",
    "        features.append(copy.copy(np.reshape(out[middle_layer_name],(middle_layer_shape[0],-1) )))\n",
    "        labels.append(copy.copy(out['label']))\n",
    "        if istest==True :\n",
    "            acc = acc + net.blobs['accuracy_test_top01'].data #accuracy_test_top01\n",
    "    acc = acc / num_batch\n",
    "    if istest==True:\n",
    "        print('Accuracy of test is {:f}'.format(acc))\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    print('Features are loaded')\n",
    "    print('Percentage of men is {:f}'.format(np.sum(labels)/labels.shape[0]))\n",
    "    return features, labels\n",
    "\n",
    "def my_pca(features, labels, num_pca):\n",
    "    mean_feature = np.mean(features, axis=0)\n",
    "    normalized_features = features - mean_feature\n",
    "    #pca = decomposition.PCA(svd_solver='randomized')\n",
    "    pca = decomposition.IncrementalPCA(batch_size=50)\n",
    "    pca.n_components = num_pca\n",
    "    pca.fit(normalized_features)\n",
    "    print('Remained variance is:') \n",
    "    print(pca.explained_variance_ratio_) \n",
    "    X_reduced = pca.transform(normalized_features)\n",
    "    print('PCA is fitted')\n",
    "    if num_pca==2:\n",
    "        plt.figure()\n",
    "        for c, i in zip(\"rgbcmyk\", [0, 1, 2, 3, 4, 5, 6]):\n",
    "            plt.scatter(X_reduced[labels == i, 0], X_reduced[labels == i, 1], c=c)\n",
    "        plt.title('PCA of features')\n",
    "    return pca, X_reduced, mean_feature\n",
    "\n",
    "def pirvacy_ek(X_reduced, X_reduced_noisy, noise_var):\n",
    "    # Compute Privacy Measure\n",
    "    # kde = KernelDensity(kernel='gaussian', bandwidth=1).fit(X_reduced_noisy)\n",
    "    numdata = X_reduced.shape[0]\n",
    "    d = np.array([X_reduced,]*numdata) - np.transpose(np.array([X_reduced_noisy,]*numdata),(1,0,2)) #order?\n",
    "    e = multivariate_normal.pdf(d,np.zeros([X_reduced.shape[1],]), noise_var*np.eye(X_reduced.shape[1]))\n",
    "    e = e.T #added now\n",
    "    r = e/np.max(e,0)\n",
    "    m = np.sum(r>.8,0)\n",
    "    ranks = 1 + numdata - rankdata(r,0)\n",
    "    trueranks = np.diag(ranks)\n",
    "    res = np.maximum(m,trueranks)\n",
    "    #res = m\n",
    "    print min(res)\n",
    "    print max(res)\n",
    "    print np.mean(res)\n",
    "    print np.std(res)\n",
    "    print sum(res==1)\n",
    "    print sum(m>trueranks)\n",
    "    print numdata\n",
    "    res = res / numdata\n",
    "    priv = np.mean(res)\n",
    "    var = np.var(res)\n",
    "    print('Expected ek-privacy is {:f}'.format(priv))\n",
    "    return priv, var\n",
    "\n",
    "def compute_acc(net, middle_layer_name, next_layer_name, reconst_all_middles, reconst_all_labels, num_batch):\n",
    "    newacc = 0\n",
    "    for i in range(0, num_batch):\n",
    "        net.blobs[middle_layer_name].data[...] = reconst_all_middles[i,:,:,:,:]\n",
    "        net.blobs['label'].data[...] = reconst_all_labels[i,:]\n",
    "        net.forward(start=next_layer_name, end='accuracy_test_top01') #accuracy_test_top01 for gender\n",
    "        newacc = newacc + net.blobs['accuracy_test_top01'].data\n",
    "    newacc = newacc / num_batch\n",
    "    print('Accuracy of test on reconstructed features from PCA is {:f}'.format(newacc))\n",
    "    return newacc\n",
    "\n",
    "def reduce_dim_test(pca, features, labels, mean_feature):\n",
    "    normalized_features = features - mean_feature\n",
    "    X_reduced = pca.transform(normalized_features)\n",
    "    print('extracted PCA from train data is fitted to test data')\n",
    "    if pca.n_components==2:\n",
    "        plt.figure()\n",
    "        for c, i in zip(\"rgbcmyk\", [0, 1, 2, 3, 4, 5, 6]):\n",
    "            plt.scatter(X_reduced[labels == i, 0], X_reduced[labels == i, 1], c=c)\n",
    "        plt.title('PCA of features')\n",
    "    return X_reduced\n",
    "\n",
    "def load_data_into_lmdb(lmdb_name, features, labels=None):\n",
    "    env = lmdb.open(lmdb_name, map_size=features.nbytes*2)\n",
    "    \n",
    "    features = features[:,:,None,None]\n",
    "    for i in range(features.shape[0]):\n",
    "        datum = caffe.proto.caffe_pb2.Datum()\n",
    "        \n",
    "        datum.channels = features.shape[1]\n",
    "        datum.height = 1\n",
    "        datum.width = 1\n",
    "        \n",
    "        if features.dtype == np.int:\n",
    "            datum.data = features[i].tostring()\n",
    "        if features.dtype == np.float: \n",
    "            datum.float_data.extend(features[i].flat)\n",
    "        else:\n",
    "            raise Exception(\"features.dtype unknown.\")\n",
    "        \n",
    "        if labels is not None:\n",
    "            datum.label = int(labels[i])\n",
    "        \n",
    "        str_id = '{:08}'.format(i)\n",
    "        with env.begin(write=True) as txn:\n",
    "            txn.put(str_id, datum.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set your network here. You should see the caffemodels folder and pick the right caffemodel and set it. \n",
    "# Also lmdb of train.prototxt should be set to lmdb of FR. Please make a copy of it and dont ruine itself.\n",
    "# Surely you will have the memory problem. Good Luck\n",
    "net = caffe.Net(addr+'train-f.prototxt', addr+'caffemodels/_iter_3000.caffemodel', caffe.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100352"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*14*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are loaded\n",
      "Percentage of men is 50.089800\n"
     ]
    }
   ],
   "source": [
    "features, labels = load_all_middle_features(net, mid_layer, 500, False) # 500 is number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pc\n",
    "pca = joblib.load(addr+'pca/pca_'+str(num_pca)+'.pkl')\n",
    "mean_feature = np.load(addr+'pca/mean_feature_'+str(num_pca)+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.ones([100352,])\n",
    "b = a - mean_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 25.55669227,  -0.11051862,  -1.44156744,  -1.31595093]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = pca.transform(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.54095407e-04,   3.92926398e-04,  -1.42494414e-05, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pca.inverse_transform(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.60998055e-04,   4.10755983e-04,  -8.79854709e-07, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = d + mean_feature\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26.50697765,  -0.08840955,  -1.43441972,  -1.3689867 ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.ones([100352,]),np.transpose(pca.components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.79629849e-04,   4.06460204e-04,  -1.49570668e-05, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones([3,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted PCA from train data is fitted to test data\n"
     ]
    }
   ],
   "source": [
    "X_reduced = reduce_dim_test(pca, features, labels, mean_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add noise to PCA\n",
    "noise = np.random.multivariate_normal(np.zeros([num_pca,]), noise_var*np.eye(num_pca), features.shape[0])\n",
    "X_reduced_noisy = X_reduced + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruct PCA\n",
    "temp_reconst = pca.inverse_transform(X_reduced_noisy) \n",
    "noisy_reconst = temp_reconst + mean_feature      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshaping to the correct form\n",
    "middle_layer_shape = net.blobs[middle_layer_name].data.shape\n",
    "reconst_all_middles = np.reshape(noisy_reconst, (num_batch,)+middle_layer_shape )\n",
    "reconst_all_labels = np.reshape(labels, (num_batch,)+(middle_layer_shape[0],) )\n",
    "print('Reconstruction is done from PCA projections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_data_into_lmdb(mid_layer+\"_lmdb\", reconst_all_middles.astype(float), reconst_all_labels) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
